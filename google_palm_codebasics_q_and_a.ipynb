{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f972f82d",
   "metadata": {},
   "source": [
    "### Basic working of Google Palm LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34aa70b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Users/Harini Balan/AppData/Local/Microsoft/WindowsApps/python3.10.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = 'your api key here' # get this free api key from https://makersuite.google.com/\n",
    "\n",
    "llm = GooglePalm(google_api_key=api_key, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b610123",
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235a80e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "essay = llm(\"write email requesting refund for electronic item\")\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227816a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765695b5",
   "metadata": {},
   "source": [
    "### Now let's load data from Codebasics FAQ csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='codebasics_faqs.csv', source_column=\"prompt\")\n",
    "\n",
    "# Store the loaded data in the 'data' variable\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd45e51",
   "metadata": {},
   "source": [
    "### Hugging Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Initialize instructor embeddings using the Hugging Face model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\")\n",
    "\n",
    "e = instructor_embeddings.embed_query(\"What is your refund policy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fab6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "e[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571c0d2",
   "metadata": {},
   "source": [
    "As you can see above, embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\". If you are curious to know about embeddings, go to youtube and search \"codebasics word embeddings\" and you will find bunch of videos with simple, intuitive explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80a28a",
   "metadata": {},
   "source": [
    "### Vector store using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c706da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create a FAISS instance for vector database from 'data'\n",
    "vectordb = FAISS.from_documents(documents=data,\n",
    "                                 embedding=instructor_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector database\n",
    "retriever = vectordb.as_retriever(score_threshold = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd58f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdocs = retriever.get_relevant_documents(\"how about job placement support?\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6b257",
   "metadata": {},
   "source": [
    "As you can see above, the retriever that was created using FAISS and hugging face embedding is now capable of pulling relavant documents from our original CSV file knowledge store. This is very powerful and it will help us further in our project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bee857",
   "metadata": {},
   "source": [
    "##### Embeddings can be created using GooglePalm too. Also for vector database you can use chromadb as well as shown below. During our experimentation, we found hugging face embeddings and FAISS to be more appropriate for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectordb = Chroma.from_documents(data,\n",
    "#                            embedding=google_palm_embeddings,\n",
    "#                            persist_directory='./chromadb')\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3d927",
   "metadata": {},
   "source": [
    "### Create RetrievalQA chain along with prompt template üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4842c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            return_source_documents=True,\n",
    "                            chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a4cf8",
   "metadata": {},
   "source": [
    "### We are all set üëçüèº Let's ask some questions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90166e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain('Do you provide job assistance and also do you provide job gurantee?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4e3e4",
   "metadata": {},
   "source": [
    "**As you can see above, the answer of question comes from two different FAQs within our csv file and it is able to pull those questions and merge them nicely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dce73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"Do you guys provide internship and also do you offer EMI payments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48970302",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chain(\"do you have javascript course?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17dc6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"Do you have plans to launch blockchain course in future?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"should I learn power bi or tableau?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"I've a MAC computer. Can I use powerbi on it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"I don't see power pivot. how can I enable it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6539e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"What is the price of your machine learning course?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
